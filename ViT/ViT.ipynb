{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_heads = dim // heads\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3)\n",
    "        self.MHA = nn.MultiheadAttention(dim, heads, batch_first=True)       #dim means input sequence's dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)   #example: https://pytorch.org/docs/stable/generated/torch.chunk.html\n",
    "        q, k, v = [token for token in qkv]\n",
    "        result = self.MHA(q,k,v, need_weights=True)[0]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.LayerNorm(dim))\n",
    "        layers.append(nn.Linear(dim, mlp_dim))\n",
    "        layers.append(nn.GELU())\n",
    "        layers.append(nn.Linear(mlp_dim, dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim):       #in paper head_dim = dim * 4\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([(Attention(dim, heads)),(FeedForward(dim, mlp_dim))]))         \n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "    def forward(self,x):\n",
    "        for attn, ffn in self.layers:           #이전의 layers를 list에서 iterating하는 방식은 cuda, cpu device 오류남\n",
    "            x = attn(x) + x\n",
    "            x = ffn(x) + x\n",
    "        return self.norm(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, num_classes, dim, depth, heads, mlp_dim, output_dim, img_dim = [3,224,224], patch_dim = [3,56,56], dim_head = 64):\n",
    "        super().__init__()\n",
    "        image_h = img_dim[1]\n",
    "        image_w = img_dim[2]\n",
    "        patch_h = patch_dim[1]\n",
    "        patch_w = patch_dim[2]\n",
    "\n",
    "        n_patches = (image_h // patch_h) * (image_w // patch_w)\n",
    "        embedding_dim = img_dim[0] * patch_h * patch_w\n",
    "\n",
    "        self.patch_dim = patch_dim\n",
    "        self.img_dim = img_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_patches = n_patches\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        #so we flatten the patches and map to D dimensions with a trainable linear projection (Eq. 1).\n",
    "        self.projection = nn.Sequential(     \n",
    "            nn.LayerNorm(embedding_dim),                            #layernorm에 대한 언급은 못찾겠음\n",
    "            nn.Linear(embedding_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "        self.cls_token =nn.Parameter(torch.randn(1, dim))\n",
    "        self.pos_embedding =nn.Parameter(torch.randn(1, n_patches+1, dim))\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
    "\n",
    "        self.classification_head = nn.Linear(dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        channels=img.shape[1]\n",
    "\n",
    "        x = img.unfold(2, self.patch_dim[1], self.patch_dim[2]).unfold(3, self.patch_dim[1], self.patch_dim[2])\n",
    "        x = x.contiguous().view(self.batch_size, channels, self.n_patches, self.patch_dim[1], self.patch_dim[2])\n",
    "        patches = x.permute(0, 2, 3, 4, 1)\n",
    "        x = patches.contiguous().view(self.batch_size, self.n_patches, self.embedding_dim)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.repeat(self.batch_size, 1, 1)\n",
    "\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embedding[:, :(self.n_patches+1)]\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = x[:,0]\n",
    "\n",
    "        x = self.classification_head(x)\n",
    "        x = self.norm(x)                    #is this order right?\n",
    "        #많은 구현체에서 norm순서나 유무 vatiation이 많았다\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(batch_size=16, num_classes=10, dim=64, depth=8, heads=64, mlp_dim=256, output_dim=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([16,3,224,224])    #(b,c,h,w)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_642/3142704713.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(outputs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379],\n",
       "        [0.0438, 0.2338, 0.1396, 0.3822, 0.0206, 0.0663, 0.0175, 0.0277, 0.0306,\n",
       "         0.0379]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "torch.Size([16, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "std10_train = datasets.STL10(\n",
    "    \"../Datasets/STL10_PyTorch/\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = train_test_split(range(len(std10_train)), test_size=0.1)\n",
    "std10_val = copy.deepcopy(std10_train)\n",
    "std10_train = Subset(std10_train, train_indices)\n",
    "std10_val = Subset(std10_val, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 500\n",
      "4500 500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indices), len(val_indices))\n",
    "print(len(std10_train), len(std10_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    std10_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    std10_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 96, 96])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std10_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(batch_size=BATCH_SIZE, num_classes=10, dim=64, depth=8, heads=64, mlp_dim=256, output_dim=10, img_dim=[3,96,96], patch_dim=[3,24,24]).to(device)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())      #weight_decay 넣으면 학습 진행 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5313214795930046\n",
      "2.463254565852029\n",
      "2.41932840858187\n",
      "2.3775292481694903\n",
      "2.3482827356883456\n",
      "2.3274914503097532\n",
      "2.313667665209089\n",
      "2.308606333392007\n",
      "2.3061716999326434\n",
      "2.3056612168039594\n",
      "2.3046116045543124\n",
      "2.30450291293008\n",
      "2.3052499566759383\n",
      "2.3044195158141\n",
      "2.30444803408214\n",
      "2.3042026145117624\n",
      "2.3035844922065736\n",
      "2.304558348655701\n",
      "2.30431923866272\n",
      "2.304315437589373\n",
      "2.3040058987481253\n",
      "2.3046348316328866\n",
      "2.3043256878852842\n",
      "2.3042466248784748\n",
      "2.303660695893424\n",
      "2.304019810472216\n",
      "2.3040760108402796\n",
      "2.3038133723395213\n",
      "2.303826837880271\n",
      "2.3039268919399807\n",
      "2.304162836074829\n",
      "2.30404965366636\n",
      "2.304058085169111\n",
      "2.303848930767604\n",
      "2.3039709448814394\n",
      "2.3039488213402883\n",
      "2.3036889723369054\n",
      "2.3037174105644227\n",
      "2.3036822523389544\n",
      "2.303731768471854\n",
      "2.303824932234628\n",
      "2.303983645779746\n",
      "2.3034564818654744\n",
      "2.303376683167049\n",
      "2.303542254652296\n",
      "2.303319081238338\n",
      "2.303509041241237\n",
      "2.303492774282183\n",
      "2.3035590750830512\n",
      "2.3036773800849915\n",
      "2.3033979075295585\n",
      "2.303585549763271\n",
      "2.303572876112802\n",
      "2.303557046822139\n",
      "2.3033667853900366\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [173], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/datasets/stl10.py:118\u001b[0m, in \u001b[0;36mSTL10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    114\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/PIL/Image.py:3103\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3101\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n\u001b[0;32m-> 3103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m(mode, size, obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, rawmode, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    loss_epoch = 0\n",
    "    model.train()\n",
    "    acc = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    loss_epoch /= len(train_loader)\n",
    "    print(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
